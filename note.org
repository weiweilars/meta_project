#+title: Note

* dataset.py
** line 58
- target
#+begin_src python
{'image_id': 9, 'annotations': [{'segmentation': [[500.49, 473.53, 599.73, 419.6, 612.67, 375.37, 608.36, 354.88, 528.54, 269.66, 457.35, 201.71, 420.67, 187.69, 389.39, 192.0, 19.42, 360.27, 1.08, 389.39, 2.16, 427.15, 20.49, 473.53]], 'area': 120057.13925, 'iscrowd': 0, 'image_id': 9, 'bbox': [1.08, 187.69, 611.59, 285.84], 'category_id': 51, 'id': 1038967}, {'segmentation': [[357.03, 69.03, 311.73, 15.1, 550.11, 4.31, 631.01, 62.56, 629.93, 88.45, 595.42, 185.53, 513.44, 230.83, 488.63, 232.99, 437.93, 190.92, 429.3, 189.84, 434.7, 148.85, 410.97, 121.89, 359.19, 74.43, 358.11, 65.8]], 'area': 44434.751099999994, 'iscrowd': 0, 'image_id': 9, 'bbox': [311.73, 4.31, 319.28, 228.68], 'category_id': 51, 'id': 1039564}, {'segmentation': [[249.6, 348.99, 267.67, 311.72, 291.39, 294.78, 304.94, 294.78, 326.4, 283.48, 345.6, 273.32, 368.19, 269.93, 385.13, 268.8, 388.52, 257.51, 393.04, 250.73, 407.72, 240.56, 425.79, 230.4, 441.6, 229.27, 447.25, 237.18, 447.25, 256.38, 456.28, 254.12, 475.48, 263.15, 486.78, 271.06, 495.81, 264.28, 498.07, 257.51, 500.33, 255.25, 507.11, 259.76, 513.88, 266.54, 513.88, 273.32, 513.88, 276.71, 526.31, 276.71, 526.31, 286.87, 519.53, 291.39, 519.53, 297.04, 524.05, 306.07, 525.18, 315.11, 529.69, 329.79, 529.69, 337.69, 530.82, 348.99, 536.47, 339.95, 545.51, 350.12, 555.67, 360.28, 557.93, 380.61, 561.32, 394.16, 565.84, 413.36, 522.92, 441.6, 469.84, 468.71, 455.15, 474.35, 307.2, 474.35, 316.24, 464.19, 330.92, 438.21, 325.27, 399.81, 310.59, 378.35, 301.55, 371.58, 252.99, 350.12]], 'area': 49577.94434999999, 'iscrowd': 0, 'image_id': 9, 'bbox': [249.6, 229.27, 316.24, 245.08], 'category_id': 56, 'id': 1058555}, {'segmentation': [[434.48, 152.33, 433.51, 184.93, 425.44, 189.45, 376.7, 195.58, 266.94, 248.53, 179.78, 290.17, 51.62, 346.66, 16.43, 366.68, 1.9, 388.63, 0.0, 377.33, 0.0, 357.64, 0.0, 294.04, 22.56, 294.37, 56.14, 300.82, 83.58, 300.82, 109.08, 289.2, 175.26, 263.38, 216.9, 243.36, 326.34, 197.52, 387.03, 172.34, 381.54, 162.33, 380.89, 147.16, 380.89, 140.06, 370.89, 102.29, 330.86, 61.94, 318.91, 48.38, 298.57, 47.41, 287.28, 37.73, 259.51, 33.85, 240.14, 32.56, 240.14, 28.36, 247.57, 24.17, 271.46, 15.13, 282.11, 13.51, 296.96, 18.68, 336.34, 55.48, 391.55, 106.81, 432.87, 147.16], [62.46, 97.21, 130.25, 69.77, 161.25, 59.12, 183.52, 52.02, 180.94, 59.12, 170.93, 78.17, 170.28, 90.76, 157.05, 95.92, 130.25, 120.78, 119.92, 129.49, 102.17, 115.29, 64.72, 119.81, 0.0, 137.89, 0.0, 120.13, 0.0, 117.87]], 'area': 24292.781700000007, 'iscrowd': 0, 'image_id': 9, 'bbox': [0.0, 13.51, 434.48, 375.12], 'category_id': 51, 'id': 1534147}, {'segmentation': [[376.2, 61.55, 391.86, 46.35, 424.57, 40.36, 441.62, 43.59, 448.07, 50.04, 451.75, 63.86, 448.07, 68.93, 439.31, 70.31, 425.49, 73.53, 412.59, 75.38, 402.92, 84.13, 387.71, 86.89, 380.8, 70.77]], 'area': 2239.2924, 'iscrowd': 0, 'image_id': 9, 'bbox': [376.2, 40.36, 75.55, 46.53], 'category_id': 55, 'id': 1913551}, {'segmentation': [[473.92, 85.64, 469.58, 83.47, 465.78, 78.04, 466.87, 72.08, 472.84, 59.59, 478.26, 47.11, 496.71, 38.97, 514.62, 40.6, 521.13, 49.28, 523.85, 55.25, 520.05, 63.94, 501.06, 72.62, 482.6, 82.93]], 'area': 1658.8913000000007, 'iscrowd': 0, 'image_id': 9, 'bbox': [465.78, 38.97, 58.07, 46.67], 'category_id': 55, 'id': 1913746}, {'segmentation': [[385.7, 85.85, 407.12, 80.58, 419.31, 79.26, 426.56, 77.94, 435.45, 74.65, 442.7, 73.66, 449.95, 73.99, 456.87, 77.94, 463.46, 83.87, 467.74, 92.77, 469.39, 104.63, 469.72, 117.15, 469.39, 135.27, 468.73, 141.86, 466.09, 144.17, 449.29, 141.53, 437.1, 136.92, 430.18, 129.67]], 'area': 3609.3030499999995, 'iscrowd': 0, 'image_id': 9, 'bbox': [385.7, 73.66, 84.02, 70.51], 'category_id': 55, 'id': 1913856}, {'segmentation': [[458.81, 24.94, 437.61, 4.99, 391.48, 2.49, 364.05, 56.1, 377.77, 73.56, 377.77, 56.1, 392.73, 41.14, 403.95, 41.14, 420.16, 39.9, 435.12, 42.39, 442.6, 46.13, 455.06, 31.17]], 'area': 2975.276, 'iscrowd': 0, 'image_id': 9, 'bbox': [364.05, 2.49, 94.76, 71.07], 'category_id': 55, 'id': 1914001}]}
#+end_src
** line 174, self.prepare
- iscrowd = 1 are used to label large groups of objects (e.g. a crowd of people )
- change the bounding box to coordinate
#+begin_src python
# original box
tensor([[  1.0800, 187.6900, 611.5900, 285.8400],
        [311.7300,   4.3100, 319.2800, 228.6800],
        [249.6000, 229.2700, 316.2400, 245.0800],
        [  0.0000,  13.5100, 434.4800, 375.1200],
        [376.2000,  40.3600,  75.5500,  46.5300],
        [465.7800,  38.9700,  58.0700,  46.6700],
        [385.7000,  73.6600,  84.0200,  70.5100],
        [364.0500,   2.4900,  94.7600,  71.0700]])
# changed box
tensor([[  1.0800, 187.6900, 612.6700, 473.5300],
        [311.7300,   4.3100, 631.0100, 232.9900],
        [249.6000, 229.2700, 565.8400, 474.3500],
        [  0.0000,  13.5100, 434.4800, 388.6300],
        [376.2000,  40.3600, 451.7500,  86.8900],
        [465.7800,  38.9700, 523.8500,  85.6400],
        [385.7000,  73.6600, 469.7200, 144.1700],
        [364.0500,   2.4900, 458.8100,  73.5600]])

#+end_src

** line 94, support
- build support dataset : within each activate category, all image information
** line 96, sample_support_samples -- final return also
- sample the positive and negative image till self.NUM_SUPP
- keep positive and negative balance support
#+begin_src python
# target: random sized by transform
{'boxes': tensor([[0.5205, 0.6888, 0.9556, 0.5955],
        [0.2635, 0.2472, 0.4989, 0.4764],
        [0.3629, 0.7329, 0.4941, 0.5106],
        [0.6606, 0.4189, 0.6789, 0.7815],
        [0.3532, 0.1326, 0.1180, 0.0969],
        [0.2269, 0.1298, 0.0907, 0.0972],
        [0.3317, 0.2269, 0.1313, 0.1469],
        [0.3571, 0.0792, 0.1481, 0.1481]]), 'labels': tensor([51, 51, 56, 51, 55, 55, 55, 55]), 'image_id': tensor([9]), 'area': tensor([136464.9531,  50507.5000,  56353.5977,  27612.7949,   2545.3291,
          1885.6064,   4102.5742,   3381.8970]), 'iscrowd': tensor([0, 0, 0, 0, 0, 0, 0, 0]), 'orig_size': tensor([480, 640]), 'size': tensor([512, 682])}

# support images
# support class ids
tensor([51, 55, 56, 32, 76, 27, 25, 57, 85, 14, 84, 52, 33, 40, 73])
# support targets
#+end_src


* How to look dataset in the new dataset
** Need to check if the new labelled image has same style as coco
** pycocotools.coco
Constructor of Microsoft COCO helper class for reading and visualizing annotations.
:param annotation_file (str): location of annotation file
:param image_folder (str): location to the folder that hosts images.
:return:


* List of tasks:
** change dataloader and dataset according to real labeled data
** evaluation step, should share the category support, but right now each iter calculate once ???!!!
** look into accuracy calculation in evaluation step
** Set val.loss to min in callback--earlystopping
** write the test_step
** Set mlflow
** Write the callback functions to output object detect test image
** download coco data to databricks
** training base model on databricks
** training novel model on databricks
